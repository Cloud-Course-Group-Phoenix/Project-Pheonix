{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cloud-Course-Group-Phoenix/Project-Pheonix/blob/Dev/Logic/BigDataPySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLRBniZYlTxv"
      },
      "source": [
        "# **Step 1 - Installation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zQ2Lb-vbbNiY"
      },
      "outputs": [],
      "source": [
        "# Install Java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Download the latest Apache Spark version\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.4.1-bin-hadoop3.tgz\n",
        "\n",
        "# Install findspark to connect Python with Spark\n",
        "!pip install -q findspark\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygI7tV_VmwkX"
      },
      "source": [
        "# **Step 2 - Environment Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JyD2EWzBikq7"
      },
      "outputs": [],
      "source": [
        "# Import the os module to interact with the operating system\n",
        "import os\n",
        "# Import findspark to locate the Spark installation\n",
        "import findspark\n",
        "\n",
        "# Set the environment variable for Java home directory (required for Spark to run)\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# Set the environment variable for Spark home directory to the downloaded Spark path\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\"\n",
        "\n",
        "# Initialize findspark to make pyspark importable within Python\n",
        "findspark.init()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq4LRLhlnxSR"
      },
      "source": [
        "# **Step 3 - Create SparkSession**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "od3pxJrQklz1"
      },
      "outputs": [],
      "source": [
        "# Import SparkSession class from PySpark SQL module\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession object, which is the entry point to use Spark functionality\n",
        "  # Set the name of the Spark application to be \"Big Data Example\"\n",
        "  # Create a new SparkSession or return an existing one\n",
        "spark = SparkSession.builder.appName(\"Big Data Example\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC-aYahaqIGG"
      },
      "source": [
        "# **Step 4 â€“ Continue with DataFrame operations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8ItNHuCkv3l",
        "outputId": "9cdf21d6-3e4a-4b07-aadb-2000374d5996"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+\n",
            "|name|price|\n",
            "+----+-----+\n",
            "| Tal|  120|\n",
            "|Dina|  150|\n",
            "+----+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define a list of tuples, each containing a name and a price\n",
        "data = [(\"Tal\", 120), (\"Uri\", 90), (\"Dina\", 150)]\n",
        "# Define the column names for the DataFrame\n",
        "columns = [\"name\", \"price\"]\n",
        "# Create a DataFrame from the data and column names using the SparkSession\n",
        "df = spark.createDataFrame(data, columns)\n",
        "# Filter the DataFrame to include only rows where the price is greater than 100\n",
        "df.filter(df[\"price\"] > 100).show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}