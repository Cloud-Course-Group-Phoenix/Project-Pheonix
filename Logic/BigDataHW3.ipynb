{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cloud-Course-Group-Phoenix/Project-Pheonix/blob/Dev/Logic/BigDataHW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import DB"
      ],
      "metadata": {
        "id": "2EEotNLCorgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys, re, operator\n",
        "\n",
        "try:\n",
        "    # Clone the GitHub repository if not already present\n",
        "    if not os.path.exists(\"/content/Project-Pheonix\"):\n",
        "        !git clone https://github.com/Cloud-Course-Group-Phoenix/Project-Pheonix.git /content/Project-Pheonix\n",
        "\n",
        "    # Change directory to project root\n",
        "    %cd /content/Project-Pheonix\n",
        "\n",
        "    # Checkout the 'dev' branch\n",
        "    !git fetch origin -q\n",
        "    !git checkout Dev -q\n",
        "\n",
        "    # Add project directory to Python path\n",
        "    sys.path.append(\"/content/Project-Pheonix/Logic\")\n",
        "    %pip install -q importnb\n",
        "    from importnb import Notebook\n",
        "    with Notebook():\n",
        "        import CloudDB as dbService\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"âŒ Setup failed:\", str(e))"
      ],
      "metadata": {
        "id": "dzD6L1smoppy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLRBniZYlTxv"
      },
      "source": [
        "# **Step 1 - Installation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zQ2Lb-vbbNiY"
      },
      "outputs": [],
      "source": [
        "# Install Java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Download the latest Apache Spark version\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.4.1-bin-hadoop3.tgz\n",
        "\n",
        "# Install findspark to connect Python with Spark\n",
        "!pip install -q findspark\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygI7tV_VmwkX"
      },
      "source": [
        "# **Step 2 - Environment Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyD2EWzBikq7"
      },
      "outputs": [],
      "source": [
        "# Import findspark to locate the Spark installation\n",
        "import findspark\n",
        "\n",
        "# Set the environment variable for Java home directory (required for Spark to run)\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# Set the environment variable for Spark home directory to the downloaded Spark path\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\"\n",
        "\n",
        "# Initialize findspark to make pyspark importable within Python\n",
        "findspark.init()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq4LRLhlnxSR"
      },
      "source": [
        "# **Step 3 - Create SparkSession**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "od3pxJrQklz1"
      },
      "outputs": [],
      "source": [
        "# Import SparkSession class from PySpark SQL module\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession object, which is the entry point to use Spark functionality\n",
        "  # Set the name of the Spark application to be \"Big Data Example\"\n",
        "  # Create a new SparkSession or return an existing one\n",
        "spark = SparkSession.builder.appName(\"Big Data Example\").getOrCreate()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}