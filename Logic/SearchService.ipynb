{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cloud-Course-Group-Phoenix/Project-Pheonix/blob/main/Logic/SearchService.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "%pip install -q importnb\n",
        "\n",
        "try:\n",
        "    #Clone the GitHub repository if not already present\n",
        "    if not os.path.exists(\"/content/Project-Pheonix\"):\n",
        "        !git clone https://github.com/Cloud-Course-Group-Phoenix/Project-Pheonix.git /content/Project-Pheonix\n",
        "\n",
        "    # Change directory to project root\n",
        "    %cd /content/Project-Pheonix\n",
        "\n",
        "    # Checkout the 'main' branch\n",
        "    !git fetch origin -q\n",
        "    !git checkout main -q\n",
        "\n",
        "    # Add project directory to Python path\n",
        "    sys.path.append(\"/content/Project-Pheonix/Logic\")\n",
        "\n",
        "    # Import notebook containing DB connection\n",
        "    from importnb import Notebook\n",
        "    with Notebook():\n",
        "        import Admin as admin\n",
        "        import CloudDB as dbService\n",
        "    from bs4 import BeautifulSoup\n",
        "    import nltk\n",
        "    import re\n",
        "    from urllib.parse import urljoin, urlparse\n",
        "    from nltk.stem import PorterStemmer\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Setup failed:\", str(e))\n",
        "\n",
        "# Site to index\n",
        "url = 'https://mqtt.org/'\n"
      ],
      "metadata": {
        "id": "tADppwcHm1ph",
        "outputId": "0b9fd391-7f07-419e-f37d-b527e8e719b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/46.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCloning into '/content/Project-Pheonix'...\n",
            "remote: Enumerating objects: 515, done.\u001b[K\n",
            "remote: Counting objects: 100% (90/90), done.\u001b[K\n",
            "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
            "remote: Total 515 (delta 55), reused 13 (delta 13), pack-reused 425 (from 1)\u001b[K\n",
            "Receiving objects: 100% (515/515), 1.89 MiB | 5.80 MiB/s, done.\n",
            "Resolving deltas: 100% (282/282), done.\n",
            "/content/Project-Pheonix\n",
            "/content/Project-Pheonix\n",
            "/content/Project-Pheonix\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rhYImrbYmaq-"
      },
      "outputs": [],
      "source": [
        "def search_word(query):\n",
        "    if not query:\n",
        "        return \"üîé Enter your search terms above\\n\\nSearch for MQTT related terms like 'broker', 'publish', 'subscribe', etc.\"\n",
        "    index = dbService.get_from_db('terms')\n",
        "    if not index:\n",
        "        return \"‚ö†Ô∏è Index not found\\n\\nNo index found in the database. Please run the indexing process first from the Admin Dashboard.\"\n",
        "\n",
        "    # Track search terms for analytics\n",
        "    admin.track_search_terms(query)\n",
        "\n",
        "    # Process the query - split into individual words\n",
        "    words = re.findall(r'\\w+', query.lower())\n",
        "\n",
        "    if not words:\n",
        "        return \"‚ö†Ô∏è Invalid search\\n\\nPlease enter valid search terms.\"\n",
        "\n",
        "    # Initialize Porter Stemmer\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    # Dictionary to track all found URLs and their related words\n",
        "    all_results = {}\n",
        "    # Dictionary to track word exact appearance counts\n",
        "    word_exact_appearances = {}\n",
        "    # Dictionary to track stemmed word total appearances\n",
        "    stemmed_word_appearances = {}\n",
        "    # Keep track of words not found\n",
        "    words_not_found = []\n",
        "    # Dictionary to track which original forms were found for each search term\n",
        "    word_to_original_forms = {}\n",
        "    # Dictionary to map URLs to exact word appearances and stemmed appearances\n",
        "    url_to_word_appearances = {}\n",
        "\n",
        "    # Try to fetch content previews from URLs - empty dictionary to be populated\n",
        "    url_content_previews = {}\n",
        "\n",
        "    # Search for each word in the index using stemming\n",
        "    for word in words:\n",
        "        # Apply stemming to the search term\n",
        "        stemmed_word = stemmer.stem(word)\n",
        "\n",
        "        if stemmed_word in index:\n",
        "            # Store the exact match appearances if the exact word exists\n",
        "            exact_appearances = 0\n",
        "            if word in index[stemmed_word]:\n",
        "                exact_appearances = index[stemmed_word][word][\"Appearances\"]\n",
        "                # Track documents where this exact word appears\n",
        "                exact_doc_ids = index[stemmed_word][word].get(\"DocIDs\", [])\n",
        "                for doc_id in exact_doc_ids:\n",
        "                    if doc_id not in url_to_word_appearances:\n",
        "                        url_to_word_appearances[doc_id] = {}\n",
        "                    if word not in url_to_word_appearances[doc_id]:\n",
        "                        url_to_word_appearances[doc_id][word] = {\"exact\": 0, \"stemmed\": 0}\n",
        "                    url_to_word_appearances[doc_id][word][\"exact\"] += exact_appearances\n",
        "\n",
        "            # Collect all URLs from all word forms with this stem\n",
        "            all_urls = set()\n",
        "            total_appearances = 0\n",
        "            related_forms = []\n",
        "\n",
        "            # Process each original word form under this stem\n",
        "            for original_word, data in index[stemmed_word].items():\n",
        "                # Add this original form to the matched forms list\n",
        "                related_forms.append(original_word)\n",
        "\n",
        "                # Get URLs and appearances\n",
        "                if \"DocIDs\" in data:\n",
        "                    doc_ids = data[\"DocIDs\"]\n",
        "                    all_urls.update(doc_ids)\n",
        "\n",
        "                    # Track stemmed appearances for each document\n",
        "                    appearances = data.get(\"Appearances\", 0)\n",
        "                    for doc_id in doc_ids:\n",
        "                        if doc_id not in url_to_word_appearances:\n",
        "                            url_to_word_appearances[doc_id] = {}\n",
        "                        if word not in url_to_word_appearances[doc_id]:\n",
        "                            url_to_word_appearances[doc_id][word] = {\"exact\": 0, \"stemmed\": 0}\n",
        "                        url_to_word_appearances[doc_id][word][\"stemmed\"] += appearances\n",
        "\n",
        "                if \"Appearances\" in data:\n",
        "                    total_appearances += data[\"Appearances\"]\n",
        "\n",
        "            # Record both the exact and total appearances for this word\n",
        "            word_exact_appearances[word] = exact_appearances\n",
        "            stemmed_word_appearances[word] = total_appearances\n",
        "            word_to_original_forms[word] = related_forms\n",
        "\n",
        "            # Add each URL to the results dictionary\n",
        "            for url in all_urls:\n",
        "                if url in all_results:\n",
        "                    if word not in all_results[url]:\n",
        "                        all_results[url].append(word)\n",
        "                else:\n",
        "                    all_results[url] = [word]\n",
        "        else:\n",
        "            words_not_found.append(word)\n",
        "\n",
        "    # Format the results\n",
        "    if not all_results:\n",
        "        related_terms = []\n",
        "        # Suggest related terms if available\n",
        "        for word in words:\n",
        "            if len(word) > 2:\n",
        "                # Look for words that start with the same letters\n",
        "                for stemmed_word in index.keys():\n",
        "                    if stemmed_word.startswith(word[:2]):\n",
        "                        for original_form in index[stemmed_word].keys():\n",
        "                            if original_form not in related_terms:\n",
        "                                related_terms.append(original_form)\n",
        "                                if len(related_terms) >= 5:  # Limit to 5 suggestions\n",
        "                                    break\n",
        "\n",
        "        no_results = f\"üòï No results found\\n\\nNo results found for: {', '.join(words)}\"\n",
        "\n",
        "        if related_terms:\n",
        "            no_results += \"\\n\\nDid you mean one of these terms?\\n\"\n",
        "            for term in related_terms[:5]:\n",
        "                no_results += f\"‚Ä¢ {term}\\n\"\n",
        "\n",
        "        return no_results\n",
        "\n",
        "    # Count the total number of URLs found\n",
        "    total_urls = len(all_results)\n",
        "\n",
        "    # Calculate both exact match appearances and stemmed appearances\n",
        "    total_exact_appearances = sum(word_exact_appearances.values())\n",
        "    total_stemmed_appearances = sum(stemmed_word_appearances.values())\n",
        "\n",
        "    # Calculate relevance score for each URL based on:\n",
        "    # 1. Number of matching search terms\n",
        "    # 2. Number of exact matches vs stemmed matches\n",
        "    # 3. Total appearances of search terms\n",
        "    url_relevance_scores = {}\n",
        "    for url, matched_words in all_results.items():\n",
        "        # Base score from number of matched words (high weight)\n",
        "        word_count_score = len(matched_words) * 50\n",
        "\n",
        "        # Score from match quality\n",
        "        exact_match_score = 0\n",
        "        stemmed_match_score = 0\n",
        "        total_match_score = 0\n",
        "\n",
        "        for word in matched_words:\n",
        "            if url in url_to_word_appearances and word in url_to_word_appearances[url]:\n",
        "                exact_count = url_to_word_appearances[url][word][\"exact\"]\n",
        "                stemmed_count = url_to_word_appearances[url][word][\"stemmed\"]\n",
        "\n",
        "                # Exact matches get higher weight\n",
        "                exact_match_score += exact_count * 3\n",
        "                stemmed_match_score += (stemmed_count - exact_count) * 1\n",
        "                total_match_score += exact_count + stemmed_count\n",
        "\n",
        "        # Combine scores with appropriate weights\n",
        "        url_relevance_scores[url] = word_count_score + exact_match_score + stemmed_match_score\n",
        "\n",
        "    # Sort results by relevance score\n",
        "    sorted_results = sorted(url_relevance_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Get content previews for the top results (limited to 10 for performance)\n",
        "    for url, _ in sorted_results[:10]:\n",
        "        try:\n",
        "            # Try to fetch the page content if not already cached\n",
        "            if url not in url_content_previews:\n",
        "                response = requests.get(url, timeout=2)  # Short timeout to prevent hanging\n",
        "                if response.status_code == 200:\n",
        "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                    # Get page title\n",
        "                    title = soup.title.string if soup.title else url.split('/')[-1]\n",
        "\n",
        "                    # Extract a relevant snippet\n",
        "                    relevant_text = \"\"\n",
        "                    paragraphs = soup.find_all(['p', 'div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
        "\n",
        "                    for p in paragraphs:\n",
        "                        text = p.get_text(strip=True)\n",
        "                        if text and any(word.lower() in text.lower() for word in words):\n",
        "                            relevant_text = text\n",
        "                            break\n",
        "\n",
        "                    # If no paragraph with search terms found, take the first non-empty paragraph\n",
        "                    if not relevant_text and paragraphs:\n",
        "                        for p in paragraphs:\n",
        "                            text = p.get_text(strip=True)\n",
        "                            if text and len(text) > 30:\n",
        "                                relevant_text = text\n",
        "                                break\n",
        "\n",
        "                    # Truncate and clean up the snippet\n",
        "                    if relevant_text:\n",
        "                        if len(relevant_text) > 200:\n",
        "                            relevant_text = relevant_text[:200] + \"...\"\n",
        "                    else:\n",
        "                        relevant_text = \"No preview available\"\n",
        "\n",
        "                    url_content_previews[url] = {\n",
        "                        \"title\": title,\n",
        "                        \"snippet\": relevant_text\n",
        "                    }\n",
        "        except Exception:\n",
        "            # On any error, provide a default preview\n",
        "            url_content_previews[url] = {\n",
        "                \"title\": url.split('/')[-1] if '/' in url else url,\n",
        "                \"snippet\": \"No preview available\"\n",
        "            }\n",
        "\n",
        "    # Build the search results as HTML for better formatting\n",
        "    result = f\"<h3>üîç Search Results</h3>\"\n",
        "    result += f\"<p>Found {len(words) - len(words_not_found)} of {len(words)} search terms across {total_urls} pages<br>\"\n",
        "    result += f\"Found {total_exact_appearances} exact matches and {total_stemmed_appearances - total_exact_appearances} related word forms</p>\"\n",
        "\n",
        "    # Add each search result as formatted HTML\n",
        "    for i, (url, _) in enumerate(sorted_results, 1):\n",
        "        matched_words = all_results[url]\n",
        "\n",
        "        # Get the preview data\n",
        "        preview = url_content_previews.get(url, {\"title\": url.split('/')[-1], \"snippet\": \"No preview available\"})\n",
        "        title = preview[\"title\"]\n",
        "        snippet = preview[\"snippet\"]\n",
        "\n",
        "        # Format the result entry as HTML with clickable link\n",
        "        result += f\"<div style='margin-bottom: 15px;'>\"\n",
        "        result += f\"<h4>{i}. {title}</h4>\"\n",
        "        result += f\"<a href='{url}' target='_blank' style='color: #1a0dab;'>{url}</a><br>\"\n",
        "        result += f\"<p>{snippet}</p>\"\n",
        "        result += f\"<p><b>Matching terms:</b> {', '.join(matched_words)}</p>\"\n",
        "        result += \"</div>\"\n",
        "\n",
        "    # Add information about words not found if any\n",
        "    if words_not_found:\n",
        "        result += f\"<p><b>Terms not found:</b> {', '.join(words_not_found)}</p>\"\n",
        "\n",
        "    return result\n"
      ]
    }
  ]
}