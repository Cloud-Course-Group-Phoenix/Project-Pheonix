{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cloud-Course-Group-Phoenix/Project-Pheonix/blob/main/Logic/Indexmqtt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhLdaxoY_rb-"
      },
      "outputs": [],
      "source": [
        "# pip installs\n",
        "!pip install firebase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPcQ1EeBANYf",
        "outputId": "91feebc1-6a42-4691-f9c0-68719dda92fb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from firebase import firebase\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "import requests\n",
        "import re\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from datetime import datetime\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('stopwords', quiet=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6So9Zja_wE_"
      },
      "outputs": [],
      "source": [
        "DBLink = \"https://couldproject-a621d-default-rtdb.europe-west1.firebasedatabase.app/\"\n",
        "url = \"https://mqtt.org/\"\n",
        "\n",
        "class DbService:\n",
        "    def __init__(self, DbLink):\n",
        "        self.db_link = DbLink\n",
        "\n",
        "    def insert_to_db(self, results, page_count):\n",
        "        FBconn = firebase.FirebaseApplication(self.db_link, None)\n",
        "        FBconn.put('/','terms', results)\n",
        "        stats = FBconn.get('/', 'indexStats') or {}\n",
        "        stats[\"word_count\"] = len(results)\n",
        "        stats[\"page_count\"] = page_count\n",
        "        stats[\"last_indexed\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        FBconn.put('/','indexStats', stats)\n",
        "        return stats[\"last_indexed\"]\n",
        "\n",
        "\n",
        "    def get_from_db(self): # change into a more general statement\n",
        "        FBconn = firebase.FirebaseApplication(self.db_link,None)\n",
        "        results = FBconn.get('/','terms')\n",
        "        return results\n",
        "\n",
        "dbService = DbService(DBLink)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msbyrW-P_v3D",
        "outputId": "98adf85b-4d62-4da1-8290-538f65f63b14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting the indexing process...\n",
            "Indexing: https://mqtt.org/getting-started/\n",
            "Indexing: https://mqtt.org/mqtt-specification/\n",
            "Indexing: https://mqtt.org/software/\n",
            "Indexing: https://mqtt.org/use-cases/\n",
            "Indexing: https://mqtt.org/faq/\n",
            "Indexing: https://mqtt.org/use-cases#automotive\n",
            "Indexing: https://mqtt.org/use-cases#logistics\n",
            "Indexing: https://mqtt.org/use-cases#manufacturing\n",
            "Indexing: https://mqtt.org/use-cases#smarthome\n",
            "Indexing: https://mqtt.org/use-cases#consumer-products\n",
            "Indexing: https://mqtt.org/use-cases#transportation\n",
            "Indexing: https://mqtt.org/legal\n",
            "Indexing: https://mqtt.org/software/#shell-script\n",
            "Indexing complete. Saving 1156 stemmed words to database.\n",
            "Index saved to database.\n"
          ]
        }
      ],
      "source": [
        "class QueryService:\n",
        "    def __init__(self,url):\n",
        "        self.url = url\n",
        "        self.stemmer = PorterStemmer()\n",
        "\n",
        "    def fetch_page(self):\n",
        "        response = requests.get(self.url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            return soup\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def index_words(self, soup, index = {}, url = ''):\n",
        "        words = re.findall(r'\\w+', soup.get_text())\n",
        "        for word in words:\n",
        "            word = word.lower()\n",
        "            # Apply stemming\n",
        "            stemmed_word = self.stemmer.stem(word)\n",
        "\n",
        "            # Check if stemmed word already exists in the index\n",
        "            if stemmed_word in index:\n",
        "                # Check if the original word is already in the index under this stem\n",
        "                if word in index[stemmed_word]:\n",
        "                    # Increment the appearances count for this specific word\n",
        "                    index[stemmed_word][word][\"Appearances\"] += 1\n",
        "                    # Add URL to DocIDs if it's not already there\n",
        "                    if url and url not in index[stemmed_word][word][\"DocIDs\"]:\n",
        "                        index[stemmed_word][word][\"DocIDs\"].append(url)\n",
        "                else:\n",
        "                    # Add this original word form to the stemmed word entry\n",
        "                    index[stemmed_word][word] = {\n",
        "                        \"Appearances\": 1,\n",
        "                        \"DocIDs\": [url] if url else []\n",
        "                    }\n",
        "            else:\n",
        "                # Initialize a new entry for this stemmed word\n",
        "                index[stemmed_word] = {\n",
        "                    word: {\n",
        "                        \"Appearances\": 1,\n",
        "                        \"DocIDs\": [url] if url else []\n",
        "                    }\n",
        "                }\n",
        "\n",
        "        return index\n",
        "\n",
        "    def remove_stop_words(self, index):\n",
        "      stop_words = set(stopwords.words('english'))\n",
        "      # Create a list of stemmed stop words\n",
        "      stemmed_stop_words = [self.stemmer.stem(stop_word) for stop_word in stop_words]\n",
        "\n",
        "      # Remove all stemmed stop words from the index\n",
        "      for stemmed_stop_word in stemmed_stop_words:\n",
        "        if stemmed_stop_word in index:\n",
        "          del index[stemmed_stop_word]\n",
        "\n",
        "      return index\n",
        "\n",
        "class Crawler:\n",
        "  def __init__(self, url):\n",
        "    self.url = url\n",
        "\n",
        "  #Fetches all sub urls from a given url\n",
        "  def get_sub_urls(self, url):\n",
        "    sub_urls = []\n",
        "    stack = [url]\n",
        "    while stack:\n",
        "      url = stack.pop()\n",
        "      response = requests.get(url)\n",
        "      response.raise_for_status()  # Raise an exception for bad responses\n",
        "      soup = BeautifulSoup(response.content, 'html.parser')\n",
        "      for link in soup.find_all('a', href=True):\n",
        "          href = link['href']\n",
        "          absolute_url = urljoin(url, href)  # Make URL absolute\n",
        "\n",
        "          if (absolute_url.startswith(url)) and (absolute_url != url) and (absolute_url not in sub_urls):\n",
        "              sub_urls.append(absolute_url)\n",
        "              stack.append(absolute_url)\n",
        "\n",
        "    return sub_urls\n",
        "\n",
        "\n",
        "def index_mqtt_website():\n",
        "    print(\"Starting the indexing process...\")\n",
        "    crawler = Crawler(url)\n",
        "    sub_urls = crawler.get_sub_urls(url)\n",
        "    print(f\"ðŸ“„ Found {len(sub_urls)} pages to index\")\n",
        "    index = {}\n",
        "    page_count = 0\n",
        "    for sub_url in sub_urls:\n",
        "        print(f\"Indexing: {sub_url}\")\n",
        "        queryService = QueryService(sub_url)\n",
        "        soup = queryService.fetch_page()\n",
        "        if soup:\n",
        "            index = queryService.index_words(soup, index, sub_url)\n",
        "            page_count += 1\n",
        "\n",
        "    # Remove stop words after building the whole index\n",
        "    index = queryService.remove_stop_words(index)\n",
        "\n",
        "    time = dbService.insert_to_db(index, page_count)\n",
        "    print(\"Index saved to database.\")\n",
        "    success_message = f\"âœ… Re-indexing complete!\\nðŸ“Š Indexed {len(index)} unique words from {page_count} pages\\nðŸ•’ Completed at: {time}\"\n",
        "    return success_message\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}