{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Cloud-Course-Group-Phoenix/Project-Pheonix/blob/main/Indexmqtt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NhLdaxoY_rb-"
   },
   "outputs": [],
   "source": [
    "# pip installs\n",
    "!pip install firebase\n",
    "\n",
    "#================================= make sure all pip installs are above this line ============================================\n",
    "\n",
    "# import to clear the installation code output\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NPcQ1EeBANYf",
    "outputId": "91feebc1-6a42-4691-f9c0-68719dda92fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from firebase import firebase\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import requests\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6So9Zja_wE_"
   },
   "outputs": [],
   "source": [
    "DBLink = \"https://couldproject-a621d-default-rtdb.europe-west1.firebasedatabase.app/\"\n",
    "url = \"https://mqtt.org/\"\n",
    "\n",
    "class DbService:\n",
    "    def __init__(self, DbLink):\n",
    "        self.db_link = DbLink\n",
    "\n",
    "    def insert_to_db(self, results, page_count):\n",
    "        try:\n",
    "            print(f\"üîÑ Connecting to database...\")\n",
    "            FBconn = firebase.FirebaseApplication(self.db_link, None)\n",
    "            \n",
    "            print(f\"üìù Saving {len(results)} terms to database...\")\n",
    "            FBconn.put('/','terms', results)\n",
    "            \n",
    "            print(f\"üìä Updating indexing statistics...\")\n",
    "            stats = FBconn.get('/', 'indexStats') or {}\n",
    "            stats[\"word_count\"] = len(results)\n",
    "            stats[\"page_count\"] = page_count\n",
    "            stats[\"last_indexed\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            FBconn.put('/','indexStats', stats)\n",
    "            \n",
    "            print(f\"‚úÖ Database operations completed successfully!\")\n",
    "            return stats[\"last_indexed\"]\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving to database: {str(e)}\")\n",
    "            print(f\"üîç Debug info - Results count: {len(results) if results else 0}, Page count: {page_count}\")\n",
    "            return None\n",
    "\n",
    "    def get_stats(self):\n",
    "        try:\n",
    "            FBconn = firebase.FirebaseApplication(self.db_link, None)\n",
    "            stats = FBconn.get('/', 'indexStats')\n",
    "            return stats\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error retrieving stats from database: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def get_from_db(self): # change into a more general statement\n",
    "        try:\n",
    "            FBconn = firebase.FirebaseApplication(self.db_link,None)\n",
    "            results = FBconn.get('/','terms')\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error retrieving data from database: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "dbService = DbService(DBLink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "msbyrW-P_v3D",
    "outputId": "98adf85b-4d62-4da1-8290-538f65f63b14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the indexing process...\n",
      "Indexing: https://mqtt.org/getting-started/\n",
      "Indexing: https://mqtt.org/mqtt-specification/\n",
      "Indexing: https://mqtt.org/software/\n",
      "Indexing: https://mqtt.org/use-cases/\n",
      "Indexing: https://mqtt.org/faq/\n",
      "Indexing: https://mqtt.org/use-cases#automotive\n",
      "Indexing: https://mqtt.org/use-cases#logistics\n",
      "Indexing: https://mqtt.org/use-cases#manufacturing\n",
      "Indexing: https://mqtt.org/use-cases#smarthome\n",
      "Indexing: https://mqtt.org/use-cases#consumer-products\n",
      "Indexing: https://mqtt.org/use-cases#transportation\n",
      "Indexing: https://mqtt.org/legal\n",
      "Indexing: https://mqtt.org/software/#shell-script\n",
      "Indexing complete. Saving 1156 stemmed words to database.\n",
      "Index saved to database.\n"
     ]
    }
   ],
   "source": [
    "class QueryService:\n",
    "    def __init__(self,url):\n",
    "        self.url = url\n",
    "        self.stemmer = PorterStemmer()\n",
    "\n",
    "    def fetch_page(self):\n",
    "        try:\n",
    "            response = requests.get(self.url, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                return soup\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Warning: HTTP {response.status_code} for {self.url}\")\n",
    "                return None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"‚ùå Error fetching {self.url}: {str(e)}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Unexpected error fetching {self.url}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def index_words(self, soup, index = {}, url = ''):\n",
    "        try:\n",
    "            words = re.findall(r'\\w+', soup.get_text())\n",
    "            for word in words:\n",
    "                word = word.lower()\n",
    "                # Apply stemming\n",
    "                stemmed_word = self.stemmer.stem(word)\n",
    "\n",
    "                # Check if stemmed word already exists in the index\n",
    "                if stemmed_word in index:\n",
    "                    # Check if the original word is already in the index under this stem\n",
    "                    if word in index[stemmed_word]:\n",
    "                        # Increment the appearances count for this specific word\n",
    "                        index[stemmed_word][word][\"Appearances\"] += 1\n",
    "                        # Add URL to DocIDs if it's not already there\n",
    "                        if url and url not in index[stemmed_word][word][\"DocIDs\"]:\n",
    "                            index[stemmed_word][word][\"DocIDs\"].append(url)\n",
    "                    else:\n",
    "                        # Add this original word form to the stemmed word entry\n",
    "                        index[stemmed_word][word] = {\n",
    "                            \"Appearances\": 1,\n",
    "                            \"DocIDs\": [url] if url else []\n",
    "                        }\n",
    "                else:\n",
    "                    # Initialize a new entry for this stemmed word\n",
    "                    index[stemmed_word] = {\n",
    "                        word: {\n",
    "                            \"Appearances\": 1,\n",
    "                            \"DocIDs\": [url] if url else []\n",
    "                        }\n",
    "                    }\n",
    "            return index\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error indexing words from {url}: {str(e)}\")\n",
    "            return index\n",
    "\n",
    "    def remove_stop_words(self, index):\n",
    "        try:\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            # Create a list of stemmed stop words\n",
    "            stemmed_stop_words = [self.stemmer.stem(stop_word) for stop_word in stop_words]\n",
    "\n",
    "            # Remove all stemmed stop words from the index\n",
    "            for stemmed_stop_word in stemmed_stop_words:\n",
    "                if stemmed_stop_word in index:\n",
    "                    del index[stemmed_stop_word]\n",
    "\n",
    "            return index\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error removing stop words: {str(e)}\")\n",
    "            return index\n",
    "\n",
    "class Crawler:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "\n",
    "    #Fetches all sub urls from a given url\n",
    "    def get_sub_urls(self, url):\n",
    "        sub_urls = []\n",
    "        stack = [url]\n",
    "        processed_urls = set()  # Track processed URLs to avoid infinite loops\n",
    "        \n",
    "        while stack:\n",
    "            current_url = stack.pop()\n",
    "            \n",
    "            # Skip if already processed\n",
    "            if current_url in processed_urls:\n",
    "                continue\n",
    "                \n",
    "            processed_urls.add(current_url)\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(current_url, timeout=10)\n",
    "                response.raise_for_status()  # Raise an exception for bad responses\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                for link in soup.find_all('a', href=True):\n",
    "                    try:\n",
    "                        href = link['href']\n",
    "                        absolute_url = urljoin(current_url, href)  # Make URL absolute\n",
    "\n",
    "                        if (absolute_url.startswith(url)) and (absolute_url != url) and (absolute_url not in sub_urls) and (absolute_url not in processed_urls):\n",
    "                            sub_urls.append(absolute_url)\n",
    "                            # Limit stack size to prevent excessive crawling\n",
    "                            if len(stack) < 100:\n",
    "                                stack.append(absolute_url)\n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ö†Ô∏è Warning: Error processing link {href}: {str(e)}\")\n",
    "                        continue\n",
    "                        \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"‚ùå Error crawling {current_url}: {str(e)}\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Unexpected error crawling {current_url}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        return sub_urls\n",
    "\n",
    "\n",
    "def index_mqtt_website():\n",
    "    try:\n",
    "        print(\"üöÄ Starting the indexing process...\")\n",
    "        crawler = Crawler(url)\n",
    "        \n",
    "        print(\"üîç Crawling website for pages...\")\n",
    "        sub_urls = crawler.get_sub_urls(url)\n",
    "        print(f\"üìÑ Found {len(sub_urls)} pages to index\")\n",
    "        \n",
    "        if not sub_urls:\n",
    "            print(\"‚ö†Ô∏è Warning: No pages found to index\")\n",
    "            return \"‚ùå No pages found to index\"\n",
    "        \n",
    "        index = {}\n",
    "        page_count = 0\n",
    "        successful_pages = 0\n",
    "        \n",
    "        for sub_url in sub_urls:\n",
    "            try:\n",
    "                print(f\"üìñ Indexing: {sub_url}\")\n",
    "                queryService = QueryService(sub_url)\n",
    "                soup = queryService.fetch_page()\n",
    "                if soup:\n",
    "                    index = queryService.index_words(soup, index, sub_url)\n",
    "                    page_count += 1\n",
    "                    successful_pages += 1\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Skipping {sub_url} - failed to fetch\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error processing {sub_url}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        if not index:\n",
    "            print(\"‚ùå No content was indexed\")\n",
    "            return \"‚ùå Failed to index any content\"\n",
    "\n",
    "        print(f\"üßπ Removing stop words from {len(index)} terms...\")\n",
    "        # Remove stop words after building the whole index\n",
    "        queryService = QueryService(url)  # Create instance for stop word removal\n",
    "        index = queryService.remove_stop_words(index)\n",
    "\n",
    "        print(f\"üíæ Saving index to database...\")\n",
    "        time = dbService.insert_to_db(index, page_count)\n",
    "        \n",
    "        if time:\n",
    "            print(\"‚úÖ Index saved to database successfully.\")\n",
    "            success_message = f\"‚úÖ Re-indexing complete!\\nüìä Indexed {len(index)} unique words from {successful_pages}/{len(sub_urls)} pages\\nüïí Completed at: {time}\"\n",
    "        else:\n",
    "            success_message = f\"‚ö†Ô∏è Indexing completed with database errors\\nüìä Processed {len(index)} unique words from {successful_pages}/{len(sub_urls)} pages\"\n",
    "        \n",
    "        return success_message\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_message = f\"‚ùå Critical error during indexing: {str(e)}\"\n",
    "        print(error_message)\n",
    "        return error_message\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMzXfNFY7oHx6TOBQOTVPWv",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
