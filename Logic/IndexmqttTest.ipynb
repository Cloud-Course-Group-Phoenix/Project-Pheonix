{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cloud-Course-Group-Phoenix/Project-Pheonix/blob/main/Logic/IndexmqttTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhLdaxoY_rb-"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "import requests\n",
        "import re\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "try:\n",
        "    # Step 1: Clone the GitHub repository if not already present\n",
        "    if not os.path.exists(\"/content/Project-Pheonix\"):\n",
        "        !git clone https://github.com/Cloud-Course-Group-Phoenix/Project-Pheonix.git /content/Project-Pheonix\n",
        "\n",
        "    # Step 2: Change directory to project root\n",
        "    %cd /content/Project-Pheonix\n",
        "\n",
        "    # Step 3: Checkout the 'main' branch (or develop if you have one)\n",
        "    !git fetch origin -q\n",
        "    !git checkout main -q\n",
        "\n",
        "    # Step 4: Add project directory to Python path\n",
        "    sys.path.append(\"/content/Project-Pheonix/Logic\")\n",
        "\n",
        "    print(\"‚úÖ Setup completed successfully.\")\n",
        "    from importnb import Notebook\n",
        "    with Notebook():\n",
        "        import CloudDB as dbService\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Setup failed:\", str(e))\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('stopwords', quiet=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msbyrW-P_v3D",
        "outputId": "98adf85b-4d62-4da1-8290-538f65f63b14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting the indexing process...\n",
            "Indexing: https://mqtt.org/getting-started/\n",
            "Indexing: https://mqtt.org/mqtt-specification/\n",
            "Indexing: https://mqtt.org/software/\n",
            "Indexing: https://mqtt.org/use-cases/\n",
            "Indexing: https://mqtt.org/faq/\n",
            "Indexing: https://mqtt.org/use-cases#automotive\n",
            "Indexing: https://mqtt.org/use-cases#logistics\n",
            "Indexing: https://mqtt.org/use-cases#manufacturing\n",
            "Indexing: https://mqtt.org/use-cases#smarthome\n",
            "Indexing: https://mqtt.org/use-cases#consumer-products\n",
            "Indexing: https://mqtt.org/use-cases#transportation\n",
            "Indexing: https://mqtt.org/legal\n",
            "Indexing: https://mqtt.org/software/#shell-script\n",
            "Indexing complete. Saving 1156 stemmed words to database.\n",
            "Index saved to database.\n"
          ]
        }
      ],
      "source": [
        "class QueryService:\n",
        "    def __init__(self,url):\n",
        "        self.url = url\n",
        "        self.stemmer = PorterStemmer()\n",
        "\n",
        "    def fetch_page(self):\n",
        "        response = requests.get(self.url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            return soup\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def index_words(self, soup, index = {}, url = ''):\n",
        "        words = re.findall(r'\\w+', soup.get_text())\n",
        "        for word in words:\n",
        "            word = word.lower()\n",
        "            # Apply stemming\n",
        "            stemmed_word = self.stemmer.stem(word)\n",
        "\n",
        "            # Check if stemmed word already exists in the index\n",
        "            if stemmed_word in index:\n",
        "                # Check if the original word is already in the index under this stem\n",
        "                if word in index[stemmed_word]:\n",
        "                    # Increment the appearances count for this specific word\n",
        "                    index[stemmed_word][word][\"Appearances\"] += 1\n",
        "                    # Add URL to DocIDs if it's not already there\n",
        "                    if url and url not in index[stemmed_word][word][\"DocIDs\"]:\n",
        "                        index[stemmed_word][word][\"DocIDs\"].append(url)\n",
        "                else:\n",
        "                    # Add this original word form to the stemmed word entry\n",
        "                    index[stemmed_word][word] = {\n",
        "                        \"Appearances\": 1,\n",
        "                        \"DocIDs\": [url] if url else []\n",
        "                    }\n",
        "            else:\n",
        "                # Initialize a new entry for this stemmed word\n",
        "                index[stemmed_word] = {\n",
        "                    word: {\n",
        "                        \"Appearances\": 1,\n",
        "                        \"DocIDs\": [url] if url else []\n",
        "                    }\n",
        "                }\n",
        "\n",
        "        return index\n",
        "\n",
        "    def remove_stop_words(self, index):\n",
        "      stop_words = set(stopwords.words('english'))\n",
        "      # Create a list of stemmed stop words\n",
        "      stemmed_stop_words = [self.stemmer.stem(stop_word) for stop_word in stop_words]\n",
        "\n",
        "      # Remove all stemmed stop words from the index\n",
        "      for stemmed_stop_word in stemmed_stop_words:\n",
        "        if stemmed_stop_word in index:\n",
        "          del index[stemmed_stop_word]\n",
        "\n",
        "      return index\n",
        "\n",
        "class Crawler:\n",
        "  def __init__(self, url):\n",
        "    self.url = url\n",
        "\n",
        "  #Fetches all sub urls from a given url\n",
        "  def get_sub_urls(self, url):\n",
        "    sub_urls = []\n",
        "    stack = [url]\n",
        "    while stack:\n",
        "      url = stack.pop()\n",
        "      response = requests.get(url)\n",
        "      response.raise_for_status()  # Raise an exception for bad responses\n",
        "      soup = BeautifulSoup(response.content, 'html.parser')\n",
        "      for link in soup.find_all('a', href=True):\n",
        "          href = link['href']\n",
        "          absolute_url = urljoin(url, href)  # Make URL absolute\n",
        "\n",
        "          if (absolute_url.startswith(url)) and (absolute_url != url) and (absolute_url not in sub_urls):\n",
        "              sub_urls.append(absolute_url)\n",
        "              stack.append(absolute_url)\n",
        "\n",
        "    return sub_urls\n",
        "\n",
        "\n",
        "def index_mqtt_website():\n",
        "    print(\"Starting the indexing process...\")\n",
        "    crawler = Crawler(url)\n",
        "    sub_urls = crawler.get_sub_urls(url)\n",
        "    print(f\"üìÑ Found {len(sub_urls)} pages to index\")\n",
        "    index = {}\n",
        "    page_count = 0\n",
        "    for sub_url in sub_urls:\n",
        "        print(f\"Indexing: {sub_url}\")\n",
        "        queryService = QueryService(sub_url)\n",
        "        soup = queryService.fetch_page()\n",
        "        if soup:\n",
        "            index = queryService.index_words(soup, index, sub_url)\n",
        "            page_count += 1\n",
        "\n",
        "    # Remove stop words after building the whole index\n",
        "    index = queryService.remove_stop_words(index)\n",
        "\n",
        "    time = dbService.insert_to_db_index(index, page_count)\n",
        "    print(\"Index saved to database.\")\n",
        "    success_message = f\"‚úÖ Re-indexing complete!\\nüìä Indexed {len(index)} unique words from {page_count} pages\\nüïí Completed at: {time}\"\n",
        "    return success_message\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}