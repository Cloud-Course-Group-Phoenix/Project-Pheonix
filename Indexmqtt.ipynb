{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzXfNFY7oHx6TOBQOTVPWv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cloud-Course-Group-Phoenix/Project-Pheonix/blob/main/Indexmqtt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NhLdaxoY_rb-"
      },
      "outputs": [],
      "source": [
        "# pip installs\n",
        "!pip install firebase\n",
        "\n",
        "#================================= make sure all pip installs are above this line ============================================\n",
        "\n",
        "# import to clear the installation code output\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from firebase import firebase\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from urllib.parse import urljoin\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPcQ1EeBANYf",
        "outputId": "91feebc1-6a42-4691-f9c0-68719dda92fb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DBLink = \"https://couldproject-a621d-default-rtdb.europe-west1.firebasedatabase.app/\"\n",
        "url = \"https://mqtt.org/\"\n",
        "\n",
        "class DbService:\n",
        "    def __init__(self, DbLink):\n",
        "        self.db_link = DbLink\n",
        "\n",
        "    def insert_to_db(self, results):\n",
        "        FBconn = firebase.FirebaseApplication(self.db_link, None)\n",
        "        FBconn.put('/','terms', results)\n",
        "\n",
        "\n",
        "    def get_from_db(self): # change into a more general statement\n",
        "        FBconn = firebase.FirebaseApplication(self.db_link,None)\n",
        "        results = FBconn.get('/','terms')\n",
        "        return results\n",
        "\n",
        "dbService = DbService(DBLink)"
      ],
      "metadata": {
        "id": "A6So9Zja_wE_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QueryService:\n",
        "    def __init__(self,url):\n",
        "        self.url = url\n",
        "        self.stemmer = PorterStemmer()\n",
        "\n",
        "    def fetch_page(self):\n",
        "        response = requests.get(self.url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            return soup\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def index_words(self, soup, index = {}, url = ''):\n",
        "        words = re.findall(r'\\w+', soup.get_text())\n",
        "        for word in words:\n",
        "            word = word.lower()\n",
        "            # Apply stemming\n",
        "            stemmed_word = self.stemmer.stem(word)\n",
        "\n",
        "            # Check if stemmed word already exists in the index\n",
        "            if stemmed_word in index:\n",
        "                # Check if the original word is already in the index under this stem\n",
        "                if word in index[stemmed_word]:\n",
        "                    # Increment the appearances count for this specific word\n",
        "                    index[stemmed_word][word][\"Appearances\"] += 1\n",
        "                    # Add URL to DocIDs if it's not already there\n",
        "                    if url and url not in index[stemmed_word][word][\"DocIDs\"]:\n",
        "                        index[stemmed_word][word][\"DocIDs\"].append(url)\n",
        "                else:\n",
        "                    # Add this original word form to the stemmed word entry\n",
        "                    index[stemmed_word][word] = {\n",
        "                        \"Appearances\": 1,\n",
        "                        \"DocIDs\": [url] if url else []\n",
        "                    }\n",
        "            else:\n",
        "                # Initialize a new entry for this stemmed word\n",
        "                index[stemmed_word] = {\n",
        "                    word: {\n",
        "                        \"Appearances\": 1,\n",
        "                        \"DocIDs\": [url] if url else []\n",
        "                    }\n",
        "                }\n",
        "\n",
        "        return index\n",
        "\n",
        "    def remove_stop_words(self, index):\n",
        "      stop_words = set(stopwords.words('english'))\n",
        "      # Create a list of stemmed stop words\n",
        "      stemmed_stop_words = [self.stemmer.stem(stop_word) for stop_word in stop_words]\n",
        "\n",
        "      # Remove all stemmed stop words from the index\n",
        "      for stemmed_stop_word in stemmed_stop_words:\n",
        "        if stemmed_stop_word in index:\n",
        "          del index[stemmed_stop_word]\n",
        "\n",
        "      return index\n",
        "\n",
        "class Crawler:\n",
        "  def __init__(self, url):\n",
        "    self.url = url\n",
        "\n",
        "  #Fetches all sub urls from a given url\n",
        "  def get_sub_urls(self, url):\n",
        "    sub_urls = []\n",
        "    stack = [url]\n",
        "    while stack:\n",
        "      url = stack.pop()\n",
        "      response = requests.get(url)\n",
        "      response.raise_for_status()  # Raise an exception for bad responses\n",
        "      soup = BeautifulSoup(response.content, 'html.parser')\n",
        "      for link in soup.find_all('a', href=True):\n",
        "          href = link['href']\n",
        "          absolute_url = urljoin(url, href)  # Make URL absolute\n",
        "\n",
        "          if (absolute_url.startswith(url)) and (absolute_url != url) and (absolute_url not in sub_urls):\n",
        "              sub_urls.append(absolute_url)\n",
        "              stack.append(absolute_url)\n",
        "\n",
        "    return sub_urls\n",
        "\n",
        "\n",
        "# Initialize crawler and build the index\n",
        "print(\"Starting the indexing process...\")\n",
        "crawler = Crawler(url)\n",
        "sub_urls = crawler.get_sub_urls(url)\n",
        "index = {}\n",
        "for sub_url in sub_urls:\n",
        "    print(f\"Indexing: {sub_url}\")\n",
        "    queryService = QueryService(sub_url)\n",
        "    soup = queryService.fetch_page()\n",
        "    if soup:\n",
        "        index = queryService.index_words(soup, index, sub_url)\n",
        "\n",
        "# Remove stop words after building the whole index\n",
        "index = queryService.remove_stop_words(index)\n",
        "\n",
        "# Save to database\n",
        "print(f\"Indexing complete. Saving {len(index)} stemmed words to database.\")\n",
        "dbService.insert_to_db(index)\n",
        "print(\"Index saved to database.\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msbyrW-P_v3D",
        "outputId": "98adf85b-4d62-4da1-8290-538f65f63b14"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting the indexing process...\n",
            "Indexing: https://mqtt.org/getting-started/\n",
            "Indexing: https://mqtt.org/mqtt-specification/\n",
            "Indexing: https://mqtt.org/software/\n",
            "Indexing: https://mqtt.org/use-cases/\n",
            "Indexing: https://mqtt.org/faq/\n",
            "Indexing: https://mqtt.org/use-cases#automotive\n",
            "Indexing: https://mqtt.org/use-cases#logistics\n",
            "Indexing: https://mqtt.org/use-cases#manufacturing\n",
            "Indexing: https://mqtt.org/use-cases#smarthome\n",
            "Indexing: https://mqtt.org/use-cases#consumer-products\n",
            "Indexing: https://mqtt.org/use-cases#transportation\n",
            "Indexing: https://mqtt.org/legal\n",
            "Indexing: https://mqtt.org/software/#shell-script\n",
            "Indexing complete. Saving 1156 stemmed words to database.\n",
            "Index saved to database.\n"
          ]
        }
      ]
    }
  ]
}